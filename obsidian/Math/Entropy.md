### Definition
* Entropy in its purest information theoretical sense is a dimensionless quantity that is used for measuring uncertainty or ignorance about the state of a system. It reflects the degree of randomness or disorder in a system.
* Entropy is greater the more random a system is.

### Free energy
Entropy (uncertainty) is the average of free-energy (surprise), see more in [[Free-energy minimization]].

